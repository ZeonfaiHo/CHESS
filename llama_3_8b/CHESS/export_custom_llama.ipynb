{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.configuration_llama import CustomLlamaConfig\n",
    "from python.modeling_llama import CustomLlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_config = AutoConfig.from_pretrained('../../base_model/Meta-Llama-3-8B-hf')\n",
    "original_model = AutoModelForCausalLM.from_pretrained('../../base_model/Meta-Llama-3-8B-hf', torch_dtype=original_config.torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CustomLlamaConfig().from_pretrained('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.activation_sparsity_type = None\n",
    "config.use_spvmm = False\n",
    "config.use_vmmsp = False\n",
    "config.use_spvmm_cpu = False\n",
    "config.use_vmmsp_cpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLlamaForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(config.torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(original_model.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_layer, layer in zip(original_model.model.layers, model.model.layers):\n",
    "    layer.self_attn.q_proj_weight_t = nn.Parameter(original_layer.self_attn.q_proj.weight.transpose(0,1).contiguous())\n",
    "    layer.self_attn.o_proj_weight_t = nn.Parameter(original_layer.self_attn.o_proj.weight.transpose(0, 1).contiguous())\n",
    "    layer.mlp.down_proj_weight_t = nn.Parameter(original_layer.mlp.down_proj.weight.transpose(0, 1).contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = torch.load('../thresholds_0_5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx, layer in enumerate(model.model.layers):\n",
    "    layer.mlp.gate_proj_states_thresholds.data = thresholds['gate_proj_states_thresholds'][layer_idx].to(config.torch_dtype)\n",
    "    layer.mlp.gate_proj_states_thresholds_2.data = thresholds['gate_proj_states_thresholds_2'][layer_idx].to(config.torch_dtype)\n",
    "    layer.self_attn.attention_inputs_thresholds.data = thresholds['attention_inputs_thresholds'][layer_idx].to(config.torch_dtype)\n",
    "    layer.self_attn.attention_outputs_thresholds.data = thresholds['attention_outputs_thresholds'][layer_idx].to(config.torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.register_for_auto_class()\n",
    "model.register_for_auto_class(AutoModelForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.save_pretrained('./model')\n",
    "model.save_pretrained('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('../../base_model/Meta-Llama-3-8B-hf')\n",
    "tokenizer.save_pretrained('./model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
